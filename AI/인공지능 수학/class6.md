# Chapter 5. 선형변환, 고윳값, 고유벡터
두 개의 벡터공간이 주어졌을 때, 이 공간들 사이의 관계는 어떻게 이해할 수 있는지. 두 공간이 **본질적으로 같다**는 것은 무슨 뜻인지. 두 공간이 **유사하다**는 것은 무슨 뜻인지. 이러한 질문에 답하기 위해 선형변환의 개념이 필요하다.

**선형변환**은 하나의 벡터 공간 V에서 자신 V 또는 다른 벡터 공간 W로 가는 함수 L로서 V의 연산(벡터합과 스칼라곱)들이 그대로 보존되는 성질을 갖고 있다. 따라서 V의 원소들 사이의 연산 값이 L에 의하여 이동된 원소들 사이의 연산 값과 같다. 예를 들어, 거울은 우리의 이미지를 좌우가 바뀐 상태로 그대로 반영하고 있다. 망원경이나 현미경은 작은 물체를 크게 확대하지만 그 모양을 그대로 유지하고 있다. 이러한 성질을 선형변환으로 설명할 수 있다.

**고윳값**은 선형대수 중에 특별한 성질을 가지고 있다. 주어진 선형변환 L:V->V이 적당한 $v \in V$의 원소를 스칼라 곱으로 변환시키는 경우를 말한다. 즉 $L\(v\)=\lambda v$을 만족한다. 이때 나오는 $\lambda$를 **고윳값**이라고 하고 v를 **고유벡터**라고 한다. 고윳값과 고유벡터는 선형변환의 특징을 반영하는 중요한 개념들이다. 머신러닝에서 등장하는 **주성분분석**(PCA)은 고윳값의 크기가 큰 순서로 나열한 후 그에 대응하는 고유벡터들을 새로운 축으로 변환하여 기존 데이터를 읽어 들이는 방법이다. 이 방법은 차원의 저주(curse of dimension)을 부분적으로 해결한 것으로 볼 수 있다(절대적인 해결은 아님).

## 선형변환
벡터공간 V에서 벡터공간 W로 가는 함수 L:V->W가
1. 모든 $u,v \in V$에 대하여 $L\(u+v\)=L\(u\)+L(v)\in W$
   1. 즉, L함수로 v+u가 들어간 값과 v따로 u따로 L함수로 들어간 값의 합과 같아야한다는 것
2. 모든 $u \in V$와 스칼라 c에 대하여 $L(cv)=cL(u)\in W$
을 만족할 때 이 L을 선형변환(linear transformation, linear map)이라고 한다. 

### 선형변환을 만드는 방법
>> 벡터공간 $V=R^n$이라고 하고 $W=R^m$이라고 하자. 행렬 A를 실수 위에서 정의된 m x n행렬이라고 하자. 그러면 $L(v)=Av \in R^m$라고 정의된 함수가 선형변환이 된다.(v는 열벡터임)

**따라서 만약 $L:R^n->R^m$이 선형변환이라고 하면 $L(v)=Av$를 만족하는 m x n행렬 A가 존재한다.**

>> 여기서 행렬 A를 선형변환 L의 **표준행렬**(standard matrix)라고 한다.

